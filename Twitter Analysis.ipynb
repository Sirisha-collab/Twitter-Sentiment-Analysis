{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "844ff77e-70a4-4de6-b90b-abbfcc2bd484",
      "cell_type": "code",
      "source": "\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score, classification_report",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "id": "6a27e7cd-4570-4d46-ad01-60963a499e30",
      "cell_type": "code",
      "source": "import pandas as pd\n\ndf = pd.read_csv(\n    \"twitter_training.csv\",\n    encoding=\"latin-1\",\n    header=None   # IMPORTANT\n)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "id": "61c0e2b8-2538-49a6-9711-218c80412cd9",
      "cell_type": "code",
      "source": "print(df.head())",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "      0            1         2  \\\n0  2401  Borderlands  Positive   \n1  2401  Borderlands  Positive   \n2  2401  Borderlands  Positive   \n3  2401  Borderlands  Positive   \n4  2401  Borderlands  Positive   \n\n                                                   3  \n0  im getting on borderlands and i will murder yo...  \n1  I am coming to the borders and I will kill you...  \n2  im getting on borderlands and i will kill you ...  \n3  im coming on borderlands and i will murder you...  \n4  im getting on borderlands 2 and i will murder ...  \n"
        }
      ],
      "execution_count": 3
    },
    {
      "id": "c47ac25d-5b92-4efc-a040-adfd39a837ab",
      "cell_type": "code",
      "source": "print(df.head())\nprint(len(df.columns))\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "      0            1         2  \\\n0  2401  Borderlands  Positive   \n1  2401  Borderlands  Positive   \n2  2401  Borderlands  Positive   \n3  2401  Borderlands  Positive   \n4  2401  Borderlands  Positive   \n\n                                                   3  \n0  im getting on borderlands and i will murder yo...  \n1  I am coming to the borders and I will kill you...  \n2  im getting on borderlands and i will kill you ...  \n3  im coming on borderlands and i will murder you...  \n4  im getting on borderlands 2 and i will murder ...  \n4\n"
        }
      ],
      "execution_count": 4
    },
    {
      "id": "6c21fa28-e9b1-4a4d-ba99-f3a52093f30d",
      "cell_type": "code",
      "source": "import pandas as pd\n\ndf = pd.read_csv(\"twitter_training.csv\", header=None)\n\ndf.columns = [\"id\", \"entity\", \"sentiment\", \"text\"]\n\nprint(df.head())\n\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "     id       entity sentiment  \\\n0  2401  Borderlands  Positive   \n1  2401  Borderlands  Positive   \n2  2401  Borderlands  Positive   \n3  2401  Borderlands  Positive   \n4  2401  Borderlands  Positive   \n\n                                                text  \n0  im getting on borderlands and i will murder yo...  \n1  I am coming to the borders and I will kill you...  \n2  im getting on borderlands and i will kill you ...  \n3  im coming on borderlands and i will murder you...  \n4  im getting on borderlands 2 and i will murder ...  \n"
        }
      ],
      "execution_count": 7
    },
    {
      "id": "cdff67c3-7dca-45ce-9e66-8dcf10549cd0",
      "cell_type": "code",
      "source": "df = df[df[\"sentiment\"].isin([\"Positive\", \"Negative\"])]\n# Convert to numeric labels\ndf[\"sentiment\"] = df[\"sentiment\"].replace({\n    \"Negative\": 0,\n    \"Positive\": 1\n})\n\nprint(df[\"sentiment\"].value_counts())\nprint(df.head())",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "<ipython-input-9-1bba627db358>:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  df[\"sentiment\"] = df[\"sentiment\"].replace({\n"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "sentiment\n0    22542\n1    20832\nName: count, dtype: int64\n     id       entity  sentiment  \\\n0  2401  Borderlands          1   \n1  2401  Borderlands          1   \n2  2401  Borderlands          1   \n3  2401  Borderlands          1   \n4  2401  Borderlands          1   \n\n                                                text  \n0  im getting on borderlands and i will murder yo...  \n1  I am coming to the borders and I will kill you...  \n2  im getting on borderlands and i will kill you ...  \n3  im coming on borderlands and i will murder you...  \n4  im getting on borderlands 2 and i will murder ...  \n"
        }
      ],
      "execution_count": 9
    },
    {
      "id": "1a35e38e-a173-413b-8c50-648505fde465",
      "cell_type": "code",
      "source": "import re\n\ndef clean_text(text):\n    text = str(text)  # ensure it's string\n    text = text.lower()  # lowercase\n    \n    # remove URLs\n    text = re.sub(r'http\\S+|www\\S+', '', text)\n    \n    # remove mentions\n    text = re.sub(r'@\\w+', '', text)\n    \n    # remove hashtag symbol (keep word)\n    text = re.sub(r'#', '', text)\n    \n    # remove numbers\n    text = re.sub(r'\\d+', '', text)\n    \n    # remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n    \n    # remove extra spaces\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    return text\n\ndf['clean_text'] = df['text'].apply(clean_text)\n\nprint(df[['text', 'clean_text']].head())\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "                                                text  \\\n0  im getting on borderlands and i will murder yo...   \n1  I am coming to the borders and I will kill you...   \n2  im getting on borderlands and i will kill you ...   \n3  im coming on borderlands and i will murder you...   \n4  im getting on borderlands 2 and i will murder ...   \n\n                                          clean_text  \n0  im getting on borderlands and i will murder yo...  \n1  i am coming to the borders and i will kill you...  \n2  im getting on borderlands and i will kill you all  \n3  im coming on borderlands and i will murder you...  \n4  im getting on borderlands and i will murder yo...  \n"
        }
      ],
      "execution_count": 10
    },
    {
      "id": "c8d8a652-b34f-40e2-82a7-41deff086695",
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 11
    },
    {
      "id": "b8a61c0f-58db-4703-9069-a2330f537644",
      "cell_type": "code",
      "source": "X = df['clean_text']\ny = df['sentiment']",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 12
    },
    {
      "id": "76bbd1a2-4b65-4212-964f-949c8ee7ab92",
      "cell_type": "code",
      "source": "X_train, X_test, y_train, y_test = train_test_split(\n    X,\n    y,\n    test_size=0.2,      # 20% test data\n    random_state=42,    # for reproducibility\n    stratify=y          # keeps class balance\n)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 13
    },
    {
      "id": "0e28a072-1c15-4f35-8348-5dd37dc8a44b",
      "cell_type": "code",
      "source": "print(\"Training Data:\", X_train.shape)\nprint(\"Testing Data:\", X_test.shape)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Training Data: (34699,)\nTesting Data: (8675,)\n"
        }
      ],
      "execution_count": 14
    },
    {
      "id": "499c9781-921c-4125-95b3-767d09f214e3",
      "cell_type": "code",
      "source": "from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(\n    max_features=5000,     # use top 5000 words\n    ngram_range=(1,2),     # unigrams + bigrams\n    stop_words='english'   # remove common stopwords\n)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 15
    },
    {
      "id": "24f3dc0b-0df8-4f5c-a846-dcffc73b4588",
      "cell_type": "code",
      "source": "# Fit on training data\nX_train_tfidf = tfidf.fit_transform(X_train)\n\n# Transform test data\nX_test_tfidf = tfidf.transform(X_test)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 16
    },
    {
      "id": "4257c157-3f65-4f82-b6cf-dda12ab0a427",
      "cell_type": "code",
      "source": "print(\"Train shape:\", X_train_tfidf.shape)\nprint(\"Test shape:\", X_test_tfidf.shape)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Train shape: (34699, 5000)\nTest shape: (8675, 5000)\n"
        }
      ],
      "execution_count": 17
    },
    {
      "id": "af0b71d7-489c-4c17-b000-f204ce08fe94",
      "cell_type": "code",
      "source": "#twitter analysis sentiment in python with Big Data Analytics and machine learning concepts",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 75
    },
    {
      "id": "9eff37e4-18ac-4daa-9349-c6520029a277",
      "cell_type": "code",
      "source": "import re, time, random, warnings\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom collections import Counter\n\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import (classification_report, confusion_matrix,\n                              accuracy_score, f1_score)\n\nimport matplotlib\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 120
    },
    {
      "id": "fdabce17-dbbe-469e-8a1d-dddbb4a376f2",
      "cell_type": "code",
      "source": "class TwitterDataGenerator:\n    \"\"\"\n    Simulates a large-scale Twitter dataset ingestion pipeline.\n    Represents a streaming / batch source (Kafka, Twitter Filtered Stream API).\n    Each tweet includes realistic noise: hashtags, mentions, URLs, emojis.\n    \"\"\"\n    POSITIVE = [\n        \"I absolutely love this product! #amazing #mustbuy\",\n        \"Great experience today, feeling blessed @brand\",\n        \"Just had the best coffee ever highly recommend\",\n        \"This new update is fantastic, everything works perfectly\",\n        \"Shoutout to the amazing team, you rock #teamwork\",\n        \"Couldn't be happier with my purchase, five stars\",\n        \"The support was incredibly helpful, issue resolved instantly\",\n        \"Beautiful day, feeling motivated and positive\",\n        \"This app changed my life, so useful and intuitive\",\n        \"Wow, what an outstanding performance #proud #winner\",\n        \"Brilliant service, super fast delivery, will order again\",\n        \"The quality is exceptional, truly blown away\",\n        \"Thrilled with the results, exceeded all expectations\",\n        \"Incredible value for money, highly recommended to everyone\",\n        \"The team went above and beyond, so grateful\",\n    ]\n    NEGATIVE = [\n        \"Terrible service, waited two hours and got nothing done\",\n        \"Worst product ever, complete waste of money #disappointed\",\n        \"App keeps crashing, this is so frustrating @support\",\n        \"Never buying from this store again, absolutely awful\",\n        \"Horrible experience, the staff were rude and unhelpful\",\n        \"Broken on arrival, no response from customer service\",\n        \"This update ruined everything, please revert #angry\",\n        \"Complete disaster, I am so disappointed with this brand\",\n        \"Scam they took my money and delivered nothing, furious\",\n        \"Zero stars, the worst thing I have ever experienced\",\n        \"Pathetic quality, fell apart after one week, disgusting\",\n        \"Unacceptable wait times, no apology, losing customers fast\",\n        \"Ripped off, product looks nothing like the pictures shown\",\n        \"Support team is useless, been waiting three weeks now\",\n        \"Absolute garbage, do not waste your money on this\",\n    ]\n    NEUTRAL = [\n        \"Just checked in at the airport, flight is on time\",\n        \"New product announced today, details available online\",\n        \"Weather is cloudy with a chance of rain this afternoon\",\n        \"Attended the conference, lots of interesting talks today\",\n        \"Package shipped, estimated delivery in three to five days\",\n        \"Reading about the latest tech trends #tech\",\n        \"Meeting rescheduled to Thursday at two pm\",\n        \"Product received, will share my thoughts after testing\",\n        \"The update is live, changelog posted on the website\",\n        \"Running some errands today, busy schedule ahead\",\n        \"New blog post published on data science best practices\",\n        \"Checking the quarterly report for this fiscal year\",\n        \"Interesting article on machine learning architectures today\",\n        \"Downloaded the latest version, setting it up now\",\n        \"Workshop happening this weekend, limited seats remaining\",\n    ]\n    TOPICS = [\"#AI\", \"#MachineLearning\", \"#Python\", \"#BigData\",\n              \"#NLP\", \"#DataScience\", \"#Cloud\", \"#Innovation\", \"#Tech\"]\n    USERS  = [f\"@user_{i}\" for i in range(1, 150)]\n\n    def generate_tweet(self, sentiment: str) -> str:\n        pool  = {\"positive\": self.POSITIVE,\n                 \"negative\": self.NEGATIVE,\n                 \"neutral\":  self.NEUTRAL}[sentiment]\n        base  = random.choice(pool)\n        if random.random() < 0.3: base += \" \" + random.choice(self.TOPICS)\n        if random.random() < 0.2: base += \" \" + random.choice(self.USERS)\n        if random.random() < 0.1:\n            base += \" https://t.co/\" + \"\".join(random.choices(\"abcdef012345\", k=8))\n        return base\n\n    def generate_dataset(self, n: int = 4000, seed: int = 42) -> pd.DataFrame:\n        random.seed(seed); np.random.seed(seed)\n        labels    = [\"positive\", \"negative\", \"neutral\"]\n        weights   = [0.40, 0.35, 0.25]\n        sentiments = random.choices(labels, weights=weights, k=n)\n        now = datetime.now()\n        records = []\n        for i, sent in enumerate(sentiments):\n            records.append({\n                \"tweet_id\":    f\"tw_{i:06d}\",\n                \"text\":        self.generate_tweet(sent),\n                \"sentiment\":   sent,\n                \"timestamp\":   now - timedelta(minutes=random.randint(0, 43200)),\n                \"retweets\":    int(np.random.poisson(5)),\n                \"likes\":       int(np.random.poisson(20)),\n                \"followers\":   int(np.random.randint(10, 100_000)),\n                \"is_verified\": random.random() < 0.05,\n            })\n        df = pd.DataFrame(records)\n        vc = df.sentiment.value_counts().to_dict()\n        print(f\"[DataGenerator] {len(df):,} tweets generated | {vc}\")\n        return df",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 121
    },
    {
      "id": "b66b53f5-173f-4029-be64-313a0080e8c6",
      "cell_type": "code",
      "source": "class BigDataProcessor:\n    \"\"\"\n    Mimics Apache Spark / Hadoop MapReduce processing with data partitioning.\n    \n    PySpark equivalents are shown in inline comments.\n    Production pipeline:\n        spark = SparkSession.builder.appName(\"TwitterSentiment\").getOrCreate()\n        df_spark = spark.createDataFrame(df)\n    \"\"\"\n    def __init__(self, num_partitions: int = 4):\n        self.num_partitions = num_partitions\n        print(f\"[BigDataProcessor] {num_partitions} partitions \"\n              f\"(simulates Spark executor cores / HDFS blocks)\")\n\n    def partition_data(self, df: pd.DataFrame) -> list:\n        size = len(df) // self.num_partitions\n        parts = [df.iloc[i*size:(i+1)*size] for i in range(self.num_partitions)]\n        print(f\"  Data partitioned → {len(parts)} shards of ~{size:,} rows\")\n        return parts\n\n    def word_frequency_mapreduce(self, df: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        MAP  : tweet → [(word, 1), …]\n        REDUCE: aggregate counts\n        \n        PySpark equivalent:\n          from pyspark.sql import functions as F\n          df.select(F.explode(F.split(\"text\", \" \")).alias(\"word\"))\n            .groupBy(\"word\").count().orderBy(\"count\", ascending=False)\n        \"\"\"\n        mapped = []\n        for text in df[\"text\"]:\n            for word in re.sub(r\"[^a-z\\s]\", \" \", text.lower()).split():\n                if len(word) > 2:\n                    mapped.append((word, 1))\n        word_counts: Counter = Counter()\n        for word, cnt in mapped:\n            word_counts[word] += cnt\n        return pd.Series(dict(word_counts)).sort_values(ascending=False)\n\n    def parallel_sentiment_stats(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Parallel groupBy + aggregation per partition then merge.\n        \n        PySpark equivalent:\n          df.groupBy(\"sentiment\").agg(\n              F.count(\"*\").alias(\"count\"),\n              F.avg(\"likes\").alias(\"avg_likes\"),\n              F.avg(\"retweets\").alias(\"avg_retweets\")\n          ).show()\n        \"\"\"\n        parts   = self.partition_data(df)\n        results = []\n        for i, part in enumerate(parts):\n            agg = part.groupby(\"sentiment\").agg(\n                count=(\"tweet_id\",\"count\"),\n                avg_likes=(\"likes\",\"mean\"),\n                avg_retweets=(\"retweets\",\"mean\"),\n                avg_followers=(\"followers\",\"mean\"),\n            ).reset_index()\n            agg[\"partition\"] = i\n            results.append(agg)\n        combined = pd.concat(results)\n        final = combined.groupby(\"sentiment\").agg(\n            total_tweets=(\"count\",\"sum\"),\n            avg_likes=(\"avg_likes\",\"mean\"),\n            avg_retweets=(\"avg_retweets\",\"mean\"),\n            avg_followers=(\"avg_followers\",\"mean\"),\n        ).reset_index()\n        return final\n\n    def streaming_window_analysis(self, df: pd.DataFrame,\n                                  window_hours: int = 6) -> pd.DataFrame:\n        \"\"\"\n        Windowed aggregation — mirrors Spark Structured Streaming.\n        \n        PySpark equivalent:\n          df.groupBy(\n              F.window(\"timestamp\", f\"{window_hours} hours\"), \"sentiment\"\n          ).count()\n        \"\"\"\n        df = df.copy()\n        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n        df[\"window\"]    = df[\"timestamp\"].dt.floor(f\"{window_hours}h\")\n        return (df.groupby([\"window\",\"sentiment\"])\n                  .size().reset_index(name=\"count\")\n                  .sort_values(\"window\"))\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 122
    },
    {
      "id": "08d3de22-58f2-4f7f-b80e-5f6c3bebc323",
      "cell_type": "code",
      "source": " #English stopwords (built-in, no NLTK needed)\nSTOPWORDS = {\n    \"i\",\"me\",\"my\",\"myself\",\"we\",\"our\",\"ours\",\"ourselves\",\"you\",\"your\",\n    \"yours\",\"he\",\"him\",\"his\",\"she\",\"her\",\"hers\",\"it\",\"its\",\"they\",\"them\",\n    \"their\",\"what\",\"which\",\"who\",\"this\",\"that\",\"these\",\"those\",\"am\",\"is\",\n    \"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"have\",\"has\",\"had\",\"do\",\"does\",\n    \"did\",\"will\",\"would\",\"could\",\"should\",\"may\",\"might\",\"shall\",\"can\",\"a\",\n    \"an\",\"the\",\"and\",\"but\",\"if\",\"or\",\"as\",\"of\",\"at\",\"by\",\"for\",\"with\",\"to\",\n    \"from\",\"in\",\"on\",\"up\",\"about\",\"into\",\"than\",\"so\",\"no\",\"not\",\"just\",\n    \"also\",\"very\",\"too\",\"then\",\"now\",\"here\",\"there\",\"when\",\"where\",\"how\",\n    \"all\",\"both\",\"each\",\"more\",\"most\",\"other\",\"some\",\"such\",\"only\",\"own\",\n    \"same\",\"any\",\"few\",\"more\",\"most\",\"other\",\"because\",\"while\",\"after\",\n}\n\ndef simple_stem(word: str) -> str:\n    \"\"\"Minimal suffix-stripping stemmer.\"\"\"\n    for suffix in [\"ing\",\"tion\",\"ness\",\"ment\",\"ful\",\"less\",\"ly\",\"er\",\"est\",\"ed\",\"s\"]:\n        if word.endswith(suffix) and len(word) - len(suffix) > 3:\n            return word[:-len(suffix)]\n    return word\n\n# Rule-based VADER-style sentiment lexicon\nPOSITIVE_WORDS = {\n    \"love\",\"great\",\"amazing\",\"fantastic\",\"wonderful\",\"excellent\",\"brilliant\",\n    \"outstanding\",\"superb\",\"perfect\",\"happy\",\"joy\",\"glad\",\"pleased\",\"thrilled\",\n    \"delighted\",\"awesome\",\"incredible\",\"fantastic\",\"good\",\"nice\",\"best\",\"enjoy\",\n    \"beautiful\",\"gorgeous\",\"magnificent\",\"impressive\",\"exceptional\",\"helpful\",\n    \"friendly\",\"efficient\",\"recommend\",\"fast\",\"easy\",\"satisfied\",\"grateful\",\n}\nNEGATIVE_WORDS = {\n    \"hate\",\"terrible\",\"awful\",\"horrible\",\"worst\",\"bad\",\"disgusting\",\"pathetic\",\n    \"useless\",\"broken\",\"fail\",\"failed\",\"failure\",\"disappoint\",\"disappointed\",\n    \"angry\",\"furious\",\"upset\",\"frustrated\",\"annoyed\",\"boring\",\"poor\",\"slow\",\n    \"expensive\",\"waste\",\"scam\",\"fraud\",\"rude\",\"unhelpful\",\"useless\",\"never\",\n    \"worst\",\"disgusting\",\"awful\",\"appalling\",\"dreadful\",\"catastrophic\",\"disaster\",\n}\nINTENSIFIERS = {\"very\",\"so\",\"extremely\",\"really\",\"absolutely\",\"totally\",\"utterly\"}\nNEGATIONS    = {\"not\",\"no\",\"never\",\"don't\",\"doesn't\",\"didn't\",\"won't\",\"can't\",\"hardly\"}\n\ndef vader_score(text: str) -> dict:\n    \"\"\"Lightweight VADER-style compound scorer.\"\"\"\n    tokens = text.lower().split()\n    pos = neg = neu = 0.0\n    negate = False\n    for i, tok in enumerate(tokens):\n        clean = re.sub(r\"[^a-z]\",\"\", tok)\n        if clean in NEGATIONS:\n            negate = True; continue\n        intensify = (i > 0 and re.sub(r\"[^a-z]\",\"\",tokens[i-1]) in INTENSIFIERS)\n        weight = 1.5 if intensify else 1.0\n        if clean in POSITIVE_WORDS:\n            if negate: neg += weight\n            else:       pos += weight\n            negate = False\n        elif clean in NEGATIVE_WORDS:\n            if negate: pos += weight * 0.5\n            else:       neg += weight\n            negate = False\n        else:\n            neu += 0.5\n    total = pos + neg + neu or 1.0\n    compound = (pos - neg) / total\n    return {\"compound\": round(compound, 4),\n            \"pos\":      round(pos / total, 4),\n            \"neg\":      round(neg / total, 4),\n            \"neu\":      round(neu / total, 4)}\n\n\nclass NLPPreprocessor:\n    \"\"\"Multi-stage NLP pipeline: clean → tokenize → remove stopwords → stem.\"\"\"\n\n    def clean_text(self, text: str) -> str:\n        text = text.lower()\n        text = re.sub(r\"http\\S+|www\\S+\", \" url \", text)\n        text = re.sub(r\"@\\w+\", \" user \", text)\n        text = re.sub(r\"#(\\w+)\", r\" \\1 \", text)\n        text = re.sub(r\"[^a-z\\s]\", \" \", text)\n        text = re.sub(r\"\\s+\", \" \", text).strip()\n        return text\n\n    def tokenize(self, text: str) -> list:\n        return [t for t in text.split()\n                if t not in STOPWORDS and len(t) > 2]\n\n    def full_pipeline(self, df: pd.DataFrame) -> pd.DataFrame:\n        print(\"[NLP] Running preprocessing pipeline …\")\n        t0 = time.time()\n        df = df.copy()\n        df[\"cleaned\"]     = df[\"text\"].apply(self.clean_text)\n        df[\"tokens\"]      = df[\"cleaned\"].apply(self.tokenize)\n        df[\"stemmed\"]     = df[\"tokens\"].apply(\n                                lambda t: \" \".join(simple_stem(w) for w in t))\n        df[\"token_count\"] = df[\"tokens\"].apply(len)\n        vader_df = df[\"text\"].apply(vader_score).apply(pd.Series)\n        df = pd.concat([df, vader_df.add_prefix(\"vader_\")], axis=1)\n        print(f\"[NLP] Done in {time.time()-t0:.2f}s | {len(df):,} records\")\n        return df",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 123
    },
    {
      "id": "591241f2-75b8-4814-a7aa-af0270063290",
      "cell_type": "code",
      "source": "class SentimentMLPipeline:\n    \"\"\"\n    5 sklearn Pipelines (TF-IDF → Classifier).\n    Demonstrates: Generative, Discriminative, Kernel, Ensemble methods.\n    \"\"\"\n    MODELS = {\n        \"Naive Bayes (Generative)\": Pipeline([\n            (\"tfidf\", TfidfVectorizer(max_features=8000, ngram_range=(1,2),\n                                      sublinear_tf=True, min_df=2)),\n            (\"clf\",   MultinomialNB(alpha=0.1)),\n        ]),\n        \"Logistic Regression\": Pipeline([\n            (\"tfidf\", TfidfVectorizer(max_features=10000, ngram_range=(1,2),\n                                      sublinear_tf=True, min_df=2)),\n            (\"clf\",   LogisticRegression(max_iter=1000, C=1.0, solver=\"lbfgs\")),\n        ]),\n        \"SGD (Online Learning)\": Pipeline([\n            (\"tfidf\", TfidfVectorizer(max_features=8000, ngram_range=(1,2),\n                                      sublinear_tf=True)),\n            (\"clf\",   SGDClassifier(loss=\"log_loss\", max_iter=100,\n                                     random_state=42, n_jobs=-1)),\n        ]),\n        \"Linear SVM (Kernel)\": Pipeline([\n            (\"tfidf\", TfidfVectorizer(max_features=10000, ngram_range=(1,2),\n                                      sublinear_tf=True, min_df=2)),\n            (\"clf\",   LinearSVC(C=1.0, max_iter=2000)),\n        ]),\n        \"Random Forest (Ensemble)\": Pipeline([\n            (\"tfidf\", TfidfVectorizer(max_features=5000, ngram_range=(1,1),\n                                      sublinear_tf=True, min_df=3)),\n            (\"clf\",   RandomForestClassifier(n_estimators=150,\n                                              random_state=42, n_jobs=-1)),\n        ]),\n    }\n\n    def __init__(self):\n        self.results = {}\n        self.best_model_name = None\n        self.best_model = None\n\n    def train_all(self, X_train, y_train, X_test, y_test):\n        print(\"\\n\" + \"═\"*62)\n        print(\"  ML MODEL TRAINING & EVALUATION\")\n        print(\"═\"*62)\n        for name, pipe in self.MODELS.items():\n            t0 = time.time()\n            pipe.fit(X_train, y_train)\n            y_pred   = pipe.predict(X_test)\n            acc  = accuracy_score(y_test, y_pred)\n            f1   = f1_score(y_test, y_pred, average=\"weighted\")\n            elapsed = time.time() - t0\n            self.results[name] = {\n                \"accuracy\": acc, \"f1\": f1,\n                \"time\": elapsed, \"pipeline\": pipe, \"y_pred\": y_pred,\n            }\n            print(f\"  {name:<35}  Acc={acc:.4f}  F1={f1:.4f}  ({elapsed:.1f}s)\")\n        self.best_model_name = max(self.results, key=lambda k: self.results[k][\"f1\"])\n        self.best_model      = self.results[self.best_model_name][\"pipeline\"]\n        print(f\"\\n  ✔ Best: {self.best_model_name} \"\n              f\"(F1={self.results[self.best_model_name]['f1']:.4f})\")\n        return self.results\n\n    def cross_validate_best(self, X, y, cv: int = 5):\n        print(f\"\\n[CV] {cv}-fold cross-validation → '{self.best_model_name}' …\")\n        scores = cross_val_score(self.best_model, X, y,\n                                 cv=cv, scoring=\"f1_weighted\", n_jobs=-1)\n        print(f\"     Fold F1s : {scores.round(4)}\")\n        print(f\"     Mean={scores.mean():.4f}  Std={scores.std():.4f}\")\n        return scores\n\n    def predict_new(self, texts: list) -> list:\n        return self.best_model.predict(texts).tolist()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 124
    },
    {
      "id": "fc565ee2-8eda-436c-9220-5937f5061fe1",
      "cell_type": "code",
      "source": "class StreamingSimulator:\n    \"\"\"\n    Simulates micro-batch tweet processing (Apache Kafka → Spark Streaming).\n    \n    Production architecture:\n      Kafka Producer → Kafka Topic → Spark Structured Streaming\n      → ML Model inference → Results Sink (Redis / Elasticsearch)\n    \"\"\"\n    def __init__(self, model: SentimentMLPipeline, prep: NLPPreprocessor):\n        self.model = model\n        self.prep  = prep\n        self.processed = 0\n\n    def process_tweet(self, tweet: str) -> dict:\n        cleaned = self.prep.clean_text(tweet)\n        v       = vader_score(tweet)\n        pred    = self.model.predict_new([cleaned])[0]\n        self.processed += 1\n        return {\"text\": tweet, \"prediction\": pred,\n                \"vader\": v[\"compound\"], \"ts\": datetime.now().isoformat()}\n\n    def run_stream(self, tweets: list, verbose: bool = True) -> pd.DataFrame:\n        print(f\"\\n[Streaming] Processing {len(tweets)} tweets in real-time …\")\n        results = [self.process_tweet(t) for t in tweets]\n        if verbose:\n            for r in results:\n                label = {\"positive\":\"✅\",\"negative\":\"❌\",\"neutral\":\"⚪\"}[r[\"prediction\"]]\n                print(f\"  {label} [{r['prediction']:<8}]  {r['text'][:55]}\")\n        return pd.DataFrame(results)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 125
    },
    {
      "id": "8a128617-9fb5-413e-aa68-0aa07ddb1331",
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 126
    },
    {
      "id": "70bdb2ce-a260-47b2-ab9a-2162ef786bc8",
      "cell_type": "code",
      "source": "from sklearn.metrics import confusion_matrix\nfrom collections import Counter\nimport numpy as np\nimport re",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 127
    },
    {
      "id": "49d37ac3-9360-4498-ab74-d6f8bc489123",
      "cell_type": "code",
      "source": "COLORS = {\"positive\":\"#2ECC71\",\"negative\":\"#E74C3C\",\"neutral\":\"#3498DB\"}\n\nplt.style.use(\"ggplot\")  # replaces sns.set_theme(style=\"darkgrid\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 128
    },
    {
      "id": "412d8df8-ded2-4e38-a96c-97fab869517a",
      "cell_type": "code",
      "source": "def plot_distribution(df, out):\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n    fig.suptitle(\"Sentiment Distribution\", fontsize=14, fontweight=\"bold\")\n\n    counts = df[\"sentiment\"].value_counts()\n\n    # Bar chart\n    axes[0].bar(counts.index, counts.values,\n                color=[COLORS[s] for s in counts.index],\n                edgecolor=\"white\")\n\n    for i, (s, v) in enumerate(counts.items()):\n        axes[0].text(i, v + max(counts.values)*0.02,\n                     f\"{v:,}\", ha=\"center\", fontsize=10, fontweight=\"bold\")\n\n    axes[0].set_title(\"Tweet Count per Sentiment\")\n    axes[0].set_xlabel(\"Sentiment\")\n\n    # Pie chart\n    axes[1].pie(counts.values,\n                labels=counts.index,\n                autopct=\"%1.1f%%\",\n                colors=[COLORS[s] for s in counts.index],\n                startangle=90,\n                wedgeprops={\"edgecolor\":\"white\",\"linewidth\":2})\n\n    axes[1].set_title(\"Proportion\")\n\n    plt.tight_layout()\n    p = f\"{out}/01_sentiment_distribution.png\"\n    plt.savefig(p, dpi=130, bbox_inches=\"tight\")\n    plt.close()\n    print(f\"  [Plot] {p}\")\n    return p\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 129
    },
    {
      "id": "af1ef2db-5149-46e6-8986-f223bf12c5e7",
      "cell_type": "code",
      "source": "def plot_model_comparison(results, out):\n    names = list(results.keys())\n    accs  = [r[\"accuracy\"] for r in results.values()]\n    f1s   = [r[\"f1\"] for r in results.values()]\n    times = [r[\"time\"] for r in results.values()]\n\n    fig, axes = plt.subplots(1, 3, figsize=(17, 5))\n    fig.suptitle(\"ML Model Comparison\", fontsize=14, fontweight=\"bold\")\n\n    for ax, vals, color, title in [\n        (axes[0], accs,  \"#3498DB\", \"Accuracy\"),\n        (axes[1], f1s,   \"#2ECC71\", \"Weighted F1\"),\n        (axes[2], times, \"#E74C3C\", \"Train Time (s)\"),\n    ]:\n        bars = ax.barh(names, vals, color=color, edgecolor=\"white\")\n        ax.set_title(title)\n\n        if title != \"Train Time (s)\":\n            ax.set_xlim(0, 1)\n\n        for bar, v in zip(bars, vals):\n            ax.text(bar.get_width() + (0.005 if title != \"Train Time (s)\" else 0.02),\n                    bar.get_y() + bar.get_height()/2,\n                    f\"{v:.3f}\" if title != \"Train Time (s)\" else f\"{v:.1f}s\",\n                    va=\"center\", fontsize=9)\n\n    plt.tight_layout()\n    p = f\"{out}/02_model_comparison.png\"\n    plt.savefig(p, dpi=130, bbox_inches=\"tight\")\n    plt.close()\n    print(f\"  [Plot] {p}\")\n    return p\n\n\ndef plot_confusion_matrix(y_test, y_pred, labels, out):\n    fig, ax = plt.subplots(figsize=(7, 6))\n    cm = confusion_matrix(y_test, y_pred, labels=labels)\n\n    im = ax.imshow(cm, cmap=\"Blues\")\n\n    # Add numbers inside cells\n    for i in range(len(labels)):\n        for j in range(len(labels)):\n            ax.text(j, i, cm[i, j],\n                    ha=\"center\", va=\"center\", color=\"black\", fontsize=11)\n\n    ax.set_xticks(np.arange(len(labels)))\n    ax.set_yticks(np.arange(len(labels)))\n    ax.set_xticklabels(labels)\n    ax.set_yticklabels(labels)\n\n    ax.set_title(\"Confusion Matrix — Best Model\", fontsize=13, fontweight=\"bold\")\n    ax.set_ylabel(\"Actual\")\n    ax.set_xlabel(\"Predicted\")\n\n    plt.colorbar(im)\n    plt.tight_layout()\n\n    p = f\"{out}/03_confusion_matrix.png\"\n    plt.savefig(p, dpi=130, bbox_inches=\"tight\")\n    plt.close()\n    print(f\"  [Plot] {p}\")\n    return p\n\n\ndef plot_vader_scores(df, out):\n    fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n    fig.suptitle(\"VADER Sentiment Scores\", fontsize=14, fontweight=\"bold\")\n\n    for sent, grp in df.groupby(\"sentiment\"):\n        axes[0].hist(grp[\"vader_compound\"],\n                     bins=30,\n                     alpha=0.65,\n                     label=sent,\n                     color=COLORS[sent],\n                     edgecolor=\"white\")\n\n    axes[0].set_title(\"Compound Score Distribution\")\n    axes[0].set_xlabel(\"VADER Compound Score\")\n    axes[0].legend()\n\n    means = df.groupby(\"sentiment\")[[\"vader_pos\",\"vader_neg\",\"vader_neu\"]].mean()\n\n    means.plot(kind=\"bar\",\n               ax=axes[1],\n               color=[\"#2ECC71\",\"#E74C3C\",\"#95A5A6\"],\n               edgecolor=\"white\")\n\n    axes[1].set_title(\"Mean VADER Components per Sentiment\")\n    axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=0)\n\n    plt.tight_layout()\n    p = f\"{out}/04_vader_scores.png\"\n    plt.savefig(p, dpi=130, bbox_inches=\"tight\")\n    plt.close()\n    print(f\"  [Plot] {p}\")\n    return p\n\n\ndef plot_streaming_window(window_df, out):\n    fig, ax = plt.subplots(figsize=(13, 5))\n\n    for sent in [\"positive\",\"negative\",\"neutral\"]:\n        sub = window_df[window_df[\"sentiment\"] == sent]\n        ax.plot(sub[\"window\"].astype(str),\n                sub[\"count\"],\n                marker=\"o\",\n                label=sent,\n                color=COLORS[sent],\n                linewidth=2)\n\n    ax.set_title(\"Streaming Window Analysis — Tweets per 6h Window\",\n                 fontsize=13, fontweight=\"bold\")\n    ax.set_xlabel(\"Time Window\")\n    ax.set_ylabel(\"Tweet Count\")\n    ax.legend()\n\n    plt.xticks(rotation=30, ha=\"right\")\n    plt.tight_layout()\n\n    p = f\"{out}/05_streaming_window.png\"\n    plt.savefig(p, dpi=130, bbox_inches=\"tight\")\n    plt.close()\n    print(f\"  [Plot] {p}\")\n    return p",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 130
    },
    {
      "id": "b23f6a76-bc54-4639-823b-a4a6f2444b6b",
      "cell_type": "code",
      "source": "def plot_top_hashtags(df, out, top_n=15):\n    all_tags = []\n\n    for text in df[\"text\"]:\n        all_tags.extend(re.findall(r\"#(\\w+)\", text.lower()))\n\n    counts = Counter(all_tags).most_common(top_n)\n\n    if not counts:\n        return None\n\n    tags, vals = zip(*counts)\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.barh(list(tags)[::-1],\n            list(vals)[::-1],\n            color=\"#9B59B6\",\n            edgecolor=\"white\")\n\n    ax.set_title(f\"Top {top_n} Hashtags\",\n                 fontsize=13,\n                 fontweight=\"bold\")\n    ax.set_xlabel(\"Count\")\n\n    plt.tight_layout()\n    p = f\"{out}/06_top_hashtags.png\"\n    plt.savefig(p, dpi=130, bbox_inches=\"tight\")\n    plt.close()\n    print(f\"  [Plot] {p}\")\n    return p\n\n\ndef plot_token_distribution(df, out):\n    fig, ax = plt.subplots(figsize=(10, 5))\n\n    for sent, grp in df.groupby(\"sentiment\"):\n        ax.hist(grp[\"token_count\"],\n                bins=20,\n                alpha=0.6,\n                label=sent,\n                color=COLORS[sent],\n                edgecolor=\"white\")\n\n    ax.set_title(\"Token Count Distribution per Sentiment\",\n                 fontsize=13,\n                 fontweight=\"bold\")\n    ax.set_xlabel(\"Tokens per Tweet\")\n    ax.set_ylabel(\"Frequency\")\n    ax.legend()\n\n    plt.tight_layout()\n\n    p = f\"{out}/07_token_distribution.png\"\n    plt.savefig(p, dpi=130, bbox_inches=\"tight\")\n    plt.close()\n    print(f\"  [Plot] {p}\")\n    return p",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 131
    },
    {
      "id": "693b8ece-9403-4b23-b452-45c6cbebbb9b",
      "cell_type": "code",
      "source": "def main():\n    OUT = \".\"\n    print(\"\\n\" + \"═\"*65)\n    print(\"  TWITTER SENTIMENT ANALYSIS\")\n    print(\"  Big Data Analytics + Machine Learning Pipeline\")\n    print(\"═\"*65 + \"\\n\")\n\n    # ── STEP 1: Data Ingestion ────────────────────────────────────────────────\n    print(\"▶ STEP 1: Data Ingestion & Generation\")\n    gen   = TwitterDataGenerator()\n    df_raw = gen.generate_dataset(n=4000)\n\n    # ── STEP 2: Big Data Processing ──────────────────────────────────────────\n    print(\"\\n▶ STEP 2: Big Data Processing (MapReduce / Spark Simulation)\")\n    bdp    = BigDataProcessor(num_partitions=4)\n    stats  = bdp.parallel_sentiment_stats(df_raw)\n    print(\"\\n  Aggregated Sentiment Stats:\\n\",\n          stats.round(2).to_string(index=False))\n    window_df = bdp.streaming_window_analysis(df_raw)\n    wf_counts = bdp.word_frequency_mapreduce(df_raw)\n    print(f\"\\n  Top-10 frequent words: {dict(wf_counts.head(10))}\")\n\n    # ── STEP 3: NLP ───────────────────────────────────────────────────────────\n    print(\"\\n▶ STEP 3: NLP Preprocessing Pipeline\")\n    prep        = NLPPreprocessor()\n    df_processed = prep.full_pipeline(df_raw)\n\n    # ── STEP 4: Train/Test Split ──────────────────────────────────────────────\n    print(\"\\n▶ STEP 4: Feature Engineering & Train/Test Split\")\n    X = df_processed[\"stemmed\"]\n    y = df_processed[\"sentiment\"]\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42, stratify=y)\n    print(f\"  Train: {len(X_train):,}  |  Test: {len(X_test):,}\")\n\n    # ── STEP 5: ML Models ─────────────────────────────────────────────────────\n    print(\"\\n▶ STEP 5: Machine Learning Training & Evaluation\")\n    ml   = SentimentMLPipeline()\n    results = ml.train_all(X_train, y_train, X_test, y_test)\n    ml.cross_validate_best(X, y, cv=5)\n\n    # ── STEP 6: Detailed Evaluation ───────────────────────────────────────────\n    print(f\"\\n▶ STEP 6: Detailed Evaluation — {ml.best_model_name}\")\n    best    = results[ml.best_model_name]\n    y_pred  = best[\"y_pred\"]\n    labels  = sorted(y.unique())\n    print(\"─\" * 55)\n    print(classification_report(y_test, y_pred))\n\n    # ── STEP 7: Real-Time Streaming ───────────────────────────────────────────\n    print(\"\\n▶ STEP 7: Real-Time Streaming Simulation (Kafka / Spark Streaming)\")\n    live_tweets = [\n        \"I absolutely love this new AI tool, it's a game changer! #AI\",\n        \"Service down again, terrible reliability, very disappointed\",\n        \"Just reading about the latest data science trends today\",\n        \"Fantastic update, everything is faster and smoother now\",\n        \"Complete scam, do NOT buy this, wasted all my money\",\n        \"Conference happening this weekend, tickets available online\",\n        \"Python ecosystem keeps improving, love the community\",\n        \"Still waiting for my order, this is completely unacceptable\",\n        \"Weather looks good today, might go for a long walk\",\n        \"Best customer support experience I have ever had, ten out of ten\",\n        \"Machine learning is truly revolutionizing every industry\",\n        \"Disgraceful response from the team, no accountability whatsoever\",\n    ]\n    streamer  = StreamingSimulator(ml, prep)\n    stream_df = streamer.run_stream(live_tweets)\n    print(f\"\\n  Processed {streamer.processed} tweets in real-time\")\n    print(f\"  Stream sentiment breakdown: \"\n          f\"{stream_df['prediction'].value_counts().to_dict()}\")\n\n    # ── STEP 8: Summary ──────────────────────────────────────────────────────\n    print(\"\\n\" + \"═\"*65)\n    print(\"  PIPELINE COMPLETE — SUMMARY\")\n    print(\"═\"*65)\n    print(f\"  Dataset        : {len(df_raw):,} tweets (simulated Big Data)\")\n    print(f\"  ML Models      : {len(results)} trained & compared\")\n    print(f\"  Best Model     : {ml.best_model_name}\")\n    print(f\"  Accuracy       : {best['accuracy']:.4f}\")\n    print(f\"  Weighted F1    : {best['f1']:.4f}\")\n    print(f\"  Live Stream    : {streamer.processed} tweets classified\")\n    print(\"═\"*65)\n\n    return ml, df_processed, stream_df\n\nif __name__ == \"__main__\":\n    ml, df, stream = main()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "\n═════════════════════════════════════════════════════════════════\n  TWITTER SENTIMENT ANALYSIS\n  Big Data Analytics + Machine Learning Pipeline\n═════════════════════════════════════════════════════════════════\n\n▶ STEP 1: Data Ingestion & Generation\n[DataGenerator] 4,000 tweets generated | {'positive': 1576, 'negative': 1426, 'neutral': 998}\n\n▶ STEP 2: Big Data Processing (MapReduce / Spark Simulation)\n[BigDataProcessor] 4 partitions (simulates Spark executor cores / HDFS blocks)\n  Data partitioned → 4 shards of ~1,000 rows\n\n  Aggregated Sentiment Stats:\n sentiment  total_tweets  avg_likes  avg_retweets  avg_followers\n negative          1426      19.85          4.97       50343.99\n  neutral           998      19.71          5.09       52531.34\n positive          1576      19.92          5.04       49202.26\n\n  Top-10 frequent words: {'the': np.int64(1396), 'this': np.int64(983), 'user': np.int64(810), 'and': np.int64(603), 'product': np.int64(419), 'today': np.int64(401), 'https': np.int64(389), 'money': np.int64(376), 'with': np.int64(353), 'ever': np.int64(316)}\n\n▶ STEP 3: NLP Preprocessing Pipeline\n[NLP] Running preprocessing pipeline …\n[NLP] Done in 0.71s | 4,000 records\n\n▶ STEP 4: Feature Engineering & Train/Test Split\n  Train: 3,200  |  Test: 800\n\n▶ STEP 5: Machine Learning Training & Evaluation\n\n══════════════════════════════════════════════════════════════\n  ML MODEL TRAINING & EVALUATION\n══════════════════════════════════════════════════════════════\n  Naive Bayes (Generative)             Acc=1.0000  F1=1.0000  (0.0s)\n  Logistic Regression                  Acc=1.0000  F1=1.0000  (0.1s)\n  SGD (Online Learning)                Acc=1.0000  F1=1.0000  (0.0s)\n  Linear SVM (Kernel)                  Acc=1.0000  F1=1.0000  (0.1s)\n  Random Forest (Ensemble)             Acc=1.0000  F1=1.0000  (0.4s)\n\n  ✔ Best: Naive Bayes (Generative) (F1=1.0000)\n\n[CV] 5-fold cross-validation → 'Naive Bayes (Generative)' …\n     Fold F1s : [1. 1. 1. 1. 1.]\n     Mean=1.0000  Std=0.0000\n\n▶ STEP 6: Detailed Evaluation — Naive Bayes (Generative)\n───────────────────────────────────────────────────────\n              precision    recall  f1-score   support\n\n    negative       1.00      1.00      1.00       285\n     neutral       1.00      1.00      1.00       200\n    positive       1.00      1.00      1.00       315\n\n    accuracy                           1.00       800\n   macro avg       1.00      1.00      1.00       800\nweighted avg       1.00      1.00      1.00       800\n\n\n▶ STEP 7: Real-Time Streaming Simulation (Kafka / Spark Streaming)\n\n[Streaming] Processing 12 tweets in real-time …\n  ✅ [positive]  I absolutely love this new AI tool, it's a game changer\n  ❌ [negative]  Service down again, terrible reliability, very disappoi\n  ⚪ [neutral ]  Just reading about the latest data science trends today\n  ✅ [positive]  Fantastic update, everything is faster and smoother now\n  ❌ [negative]  Complete scam, do NOT buy this, wasted all my money\n  ⚪ [neutral ]  Conference happening this weekend, tickets available on\n  ✅ [positive]  Python ecosystem keeps improving, love the community\n  ✅ [positive]  Still waiting for my order, this is completely unaccept\n  ⚪ [neutral ]  Weather looks good today, might go for a long walk\n  ❌ [negative]  Best customer support experience I have ever had, ten o\n  ⚪ [neutral ]  Machine learning is truly revolutionizing every industr\n  ❌ [negative]  Disgraceful response from the team, no accountability w\n\n  Processed 12 tweets in real-time\n  Stream sentiment breakdown: {'positive': 4, 'negative': 4, 'neutral': 4}\n\n═════════════════════════════════════════════════════════════════\n  PIPELINE COMPLETE — SUMMARY\n═════════════════════════════════════════════════════════════════\n  Dataset        : 4,000 tweets (simulated Big Data)\n  ML Models      : 5 trained & compared\n  Best Model     : Naive Bayes (Generative)\n  Accuracy       : 1.0000\n  Weighted F1    : 1.0000\n  Live Stream    : 12 tweets classified\n═════════════════════════════════════════════════════════════════\n"
        }
      ],
      "execution_count": 141
    },
    {
      "id": "79446f70-420a-4d88-9f17-272b3e906ac3",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}